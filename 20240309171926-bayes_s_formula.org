:PROPERTIES:
:ID:       57884b57-bec0-4369-88e5-0af24fe9cc58
:END:
#+title: Bayes's Theorem
#+filetags: conditional_probability

Suppose \(E\) and \(F\) are events in a [[id:33838eb4-aa50-4794-baa1-637ddea744ad][sample space]] S.
Then \(E = EF \cup EF^{c}\) with \(EF\) and \(EF^{c}\) [[id:b804e882-b52f-49fd-a577-bcae712bbb75][mutually exclusive]].
\(P(E)=P(EF)+P(EF^{c})\)

From the [[id:116e901f-b06c-4ab7-9793-6f1445edac41][multiplication rule]]:
\begin{equation*}
P(E) = P(E|F)P(F) + P(E|F^{c})P(F^{c})
\end{equation*}

Here \(P(E)\) is expressed as a *weighted average* of \(P(E|F)\) and \(P(E|F^{c})\).

We *condition* the probability of \(E\) on whether \(F\) occurs or not.

We use this conditioning if it is easier to find \(P(E|F)\) and \(P(E|F^{c})\) than \(P(E)\) directly.

* In General
If \(F_1, F_2, \cdots F_n\) are mutually exclusive, and \(\bigcup_{i=1}^nF_i = S\), then
\begin{equation*}
P(F_j|E) = \frac{P(E|F_j)P(F_j)}{\sum_{i=1}^nP(E|F_i)P(F_i)}, \text{ for } j = 1, 2,\cdots n
\end{equation*}
